{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5d18f990",
   "metadata": {},
   "source": [
    "# Predicting the Weekly Gross for Broadway shows\n",
    "#### Sidorenko Elena"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "919e699d",
   "metadata": {},
   "source": [
    "## Getting Started"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bff462e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd    \n",
    "import seaborn as sns   \n",
    "import matplotlib.pyplot as plt \n",
    "import numpy as np   \n",
    "import datetime as dt\n",
    "import matplotlib.image as mpimg\n",
    "import statsmodels.api as sm "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1a8a9c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading csv file\n",
    "df = pd.read_csv('broadway.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b48a3888",
   "metadata": {},
   "source": [
    "## Data inspection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3590feab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspecting the data\n",
    "df.head(3) \n",
    "df.info() \n",
    "# can already see that there are missing observations in 'potential_gross' and 'top_ticket_price'. This can later be cleaned\n",
    "# week_ending should be set as date type\n",
    "# potentially setting 'show' as a category to later group top shows\n",
    "# potentially setting 'theatre' as a category to later group top theatres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7175be4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16a599dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['show'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c63fdb48",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['week_ending'].min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78f02d7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['week_ending'].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93568f91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# maximum number of weeks each show was performed for\n",
    "max_weeks = df['week_number'].max()\n",
    "max_show = df.loc[df['week_number'] == max_weeks, 'show'].iloc[0]\n",
    "\n",
    "print(f\"The show with the maximum number of weeks is '{max_show}' with {max_weeks} weeks.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faa8d645",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data dictionary\n",
    "img         = mpimg.imread('DataDictionary.PNG') \n",
    "imgplot     = plt.imshow(img)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fe6ef0f",
   "metadata": {},
   "source": [
    "## Preparing the data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dd9a3a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1.Number of observations to start with\n",
    "df.shape[0] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a41e0e96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2. Changing variable names\n",
    "df = df.rename(columns={'pct_capacity':'percent_capacity'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "015e3508",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3. Changing variable type\n",
    "\n",
    "# Generically\n",
    "df['show']         = df['show'].astype('category')\n",
    "df['theatre']      = df['theatre'].astype('category')\n",
    "\n",
    "# Data-time variables\n",
    "df['week_ending']  = pd.to_datetime(df['week_ending'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62903189",
   "metadata": {},
   "source": [
    "### Dealing with categorical variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43797020",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4. Categorical variables 1: Changing value of observation \n",
    "# no issues found to perform this step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "283d178e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5. Checking how many unique shows and theatres there are\n",
    "\n",
    "unique_shows       = df['show'].nunique()\n",
    "unique_theatres    = df['theatre'].nunique()\n",
    "print('unique shows: ', unique_shows,' unique theatres: ', unique_theatres)\n",
    "# there are a lot of theatres and even more shows. It is best to either group each into top 10 and others, OR only take the top 10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "219c3b29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 6. Getting the top 10 most frequently performing shows, and top 10 theatres that had the most showes performed\n",
    "\n",
    "top10_shows         = df['show'].value_counts().head(10)\n",
    "top10_theatres      = df['theatre'].value_counts().head(10)\n",
    "\n",
    "top10_shows_list    = list(top10_shows.index)\n",
    "top10_theatres_list = list(top10_theatres.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e29e028",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 7. Categorical variables 2: Binarizing categorical variables (where this makes sense!)\n",
    "\n",
    "# binarising categorical variables. Setting top 10 most performed theatres as 1 and the rest as 0\n",
    "substrings_theatres   = \"|\".join(top10_theatres_list)\n",
    "df['top_10_theatres'] = np.where(df['theatre'].str.contains('|'.join(substrings_theatres)), 1, 0)\n",
    "\n",
    "# binarising categorical variables. Setting top 100 most performed shows as 1 and the rest as 0\n",
    "substrings_shows      = \"|\".join(top10_shows_list)\n",
    "df['top_10_shows']    = np.where(df['show'].str.contains('|'.join(substrings_shows)), 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ffc9d5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df['show'].value_counts().head(100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f079a1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 8. Categorical variables 3: removing observations where values are not relevant for case at hand\n",
    "df = df.groupby('show').filter(lambda x: len(x) >= 78) \n",
    "\n",
    "# Some shows were performed too little times\n",
    "# we are intersted in shows that got a lot of views and therfore public atention\n",
    "# let's work with the top 300 shows\n",
    "# we need to get rid of any show with less than 95 observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf46bde9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df['show'].nunique())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91b5af37",
   "metadata": {},
   "source": [
    "### Checking whether to remove or replace missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cc664db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 9. Categorical variables 4: Generating empty values where value does not make sense\n",
    "# petrol\n",
    "df_backup = df\n",
    "\n",
    "# Filling Null potential_gross values with seats_in_theatre * avg_ticket_price\n",
    "df_backup['potential_gross'].fillna(df_backup.apply(lambda x: x['seats_in_theatre'] * x['avg_ticket_price'], axis=1), inplace=True)\n",
    "\n",
    "# adding new column that shows whether the potential gross equals to seats * average ticket price \n",
    "df_backup['is_potential_gross_correct'] = np.where(df_backup['potential_gross'] == df_backup['seats_in_theatre'] * df_backup['avg_ticket_price'], 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecab6c6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking whether there are any 'potential_gross' that is not equal to 'seats_in_theatre'*\"avg_ticket_price\"\n",
    "(df_backup['potential_gross'] == df_backup['seats_in_theatre'] * df_backup['avg_ticket_price']).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4c1d78b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking if the potential_gross is simply seats_in_theatre * avg_ticket_price\n",
    "print(df_backup['is_potential_gross_correct'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15d9a4a2",
   "metadata": {},
   "source": [
    "So all of the potential gross prices are not that equation. Therefore we can remove the missing values. Or try and predict them. But it's not an important.Thank gosh that was in a backup dataset. We can create a new dataset, and remove all null values from the original dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a2d0a74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 10. Dropping unnecessary columns\n",
    "df = df.drop('performances', axis=1)\n",
    "df = df.drop('percent_capacity', axis=1)\n",
    "df = df.drop('weekly_gross_overall', axis=1)\n",
    "df = df.drop('is_potential_gross_correct', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5e53580",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 11. Dropping missing values\n",
    "df = df.dropna(subset=['top_ticket_price'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ce83b71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display df\n",
    "pd.options.display.max_columns = None \n",
    "df.head(2) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e7689c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f65361f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 12. Inspect the number of observations left\n",
    "df.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02b96915",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 13. Save cleaned data set\n",
    "df.to_csv('Cleaned_Broadway.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de019bc2",
   "metadata": {},
   "source": [
    "# Data is cleaned, Next step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61d64d23",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd     \n",
    "import numpy as np    \n",
    "import statsmodels.api as sm\n",
    "import seaborn as sns\n",
    " \n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import cross_val_score\n",
    "import numpy as np\n",
    " \n",
    "import matplotlib.pyplot as plt\n",
    "df = pd.read_csv('Cleaned_Broadway.csv')\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b73372d9",
   "metadata": {},
   "source": [
    "## Summary Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6521e1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('Cleaned_Broadway.csv')\n",
    "df['week_ending'] = pd.to_datetime(df['week_ending'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79870a82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary statistics\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebe1235b",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(font_scale = 1, style = 'whitegrid')\n",
    "sns.histplot(df['week_number'], color = 'orange', kde = True, bins=50)\n",
    "plt.title('Distribution of Week Numbers')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4267ca13",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(font_scale=1, style='whitegrid')\n",
    "plt.title('Distribution of Weekly Gross')\n",
    "sns.histplot(data = df, x='weekly_gross', color = 'orange', bins=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04b064c1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "70115ce4",
   "metadata": {},
   "source": [
    "## Getting Tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80e998dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install tweepy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aa74f59",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "import string\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import tweepy\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer \n",
    "from sklearn.feature_extraction.text import TfidfTransformer \n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB \n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d71007e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accessing Twitter \n",
    "consumer_key    = 'wI9UK5H6w9fr3BxqQ1tLajzFu'\n",
    "consumer_secret = 'DgrXdbgiorNNLobKjMLT1ZHdk9MYng9bahIwUA8L9TUpkuTbFc'\n",
    "access_key      = '1618892347487535104-kxehJMwndfnwrYjBnGfoei6LPSLk6S'\n",
    "access_secret   = 'zARciIULKUtQLzDpd7IRfG2xvN8VAAmBdJimC8FfZ9G7S'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41f33f28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Twitter authentication \n",
    "auth            = tweepy.OAuthHandler(consumer_key, consumer_secret)\n",
    "auth.set_access_token(access_key, access_secret)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8469ec92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating an API object\n",
    "api             = tweepy.API(auth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7c4e5cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### looping though several objects to extract tweets\n",
    "def extract_tweets (api, show):\n",
    "    show_tweets  = api.search_tweets(show + 'Broadway OR theatre', lang = 'en', tweet_mode = 'extended', count = 30)\n",
    "    tweet_list   = []\n",
    "    for tweet in show_tweets:\n",
    "        tweet_list.append({\n",
    "            'show'           : show, # the first 'show' is the column name, the second show is the actual data of the column\n",
    "            'text'            : tweet.full_text,\n",
    "            'Favorite count'  : tweet.favorite_count,\n",
    "            'Retweet count'   : tweet.retweet_count,\n",
    "            'Created at'      : tweet.created_at\n",
    "        })\n",
    "    return(tweet_list)\n",
    "\n",
    "#### Looping through the shows to compile dataframe\n",
    "shows_list       = list(df['show'].unique())\n",
    "tweets           = []\n",
    "\n",
    "for show in shows_list:\n",
    "    show_tweets  = extract_tweets(api, show)\n",
    "    tweets.extend(show_tweets)\n",
    "tweets           = pd.DataFrame(tweets)\n",
    "tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e7e8b4c",
   "metadata": {},
   "source": [
    "### Normalization of Tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eca9e75",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string \n",
    "from   nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac89500d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    # remove url and mentions (@elonmusk)\n",
    "    text_clean = re.sub(r\"hhtp\\S+\",\"\", text)\n",
    "    text_clean = re.sub(r\"@\\S+\",\"\",text_clean)\n",
    "        \n",
    "    # Remove punctuation\n",
    "    text_clean = text_clean.translate(str.maketrans(\"\",\"\",string.punctuation))\n",
    "    \n",
    "    # Replace digits with the word number \n",
    "    text_clean = re.sub(r'\\b\\d+\\b', 'number', text_clean)\n",
    "    \n",
    "    \n",
    "    # Stop words 1: Tokenize the text into words\n",
    "    words      = text_clean.split()\n",
    "    \n",
    "    # Stop words 2: Remove the stop words\n",
    "    stop_words = set(stopwords.words(\"english\"))\n",
    "    words      = [word for word in words if word.lower() not in stop_words]\n",
    "    \n",
    "    # Stop words 3: Join the words back into a single string\n",
    "    text_clean = \" \".join(words)\n",
    "    \n",
    "    return text, text_clean\n",
    "\n",
    "tweets[['text', 'text_clean']]    = tweets['text'].apply(preprocess_text).apply(pd.Series)\n",
    "tweets\n",
    "\n",
    "#### saving everything to a csv\n",
    "tweets.to_csv('tweets.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bc83a87",
   "metadata": {},
   "source": [
    "### Vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab05fc62",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df5ccb92",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets             = pd.read_csv('tweets.csv')\n",
    "tweets.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eba3de88",
   "metadata": {},
   "outputs": [],
   "source": [
    "bow_transformer    = CountVectorizer(analyzer = preprocess_text).fit(tweets['text_clean'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45099353",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the total number of words\n",
    "print(len(bow_transformer.vocabulary_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18cc0aed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformation on entire dataframe\n",
    "tweets_bow         = bow_transformer.transform(tweets['text_clean'])  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8e43362",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "tf_idf_transformer = TfidfTransformer().fit(tweets_bow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62ea4e19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply transformation to the entire bag of words\n",
    "tweets_tfidf       = tf_idf_transformer.transform(tweets_bow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10e49bf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tweets_tfidf.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87753950",
   "metadata": {},
   "source": [
    "### Natural Language Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95e10271",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaeae6a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en_core_web_md')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f19bfe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m spacy download en_core_web_md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b374929",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "817d73b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "sid = SentimentIntensityAnalyzer()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57f3fb7a",
   "metadata": {},
   "source": [
    "### Applying Vader to Tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b157e6a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets['scores']     = tweets['text_clean'].apply(lambda review: sid.polarity_scores(review))\n",
    "tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7b67215",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets['negative']   = tweets['scores'].apply(lambda x: x['neg'])\n",
    "tweets['neutral']    = tweets['scores'].apply(lambda x: x['neu'])\n",
    "tweets['positive']   = tweets['scores'].apply(lambda x: x['pos'])\n",
    "tweets['compound']   = tweets['scores'].apply(lambda x: x['compound'])\n",
    "\n",
    "tweets['comp_score'] = tweets['compound'].apply(lambda x: 'pos' if x >= 0 else 'neg')\n",
    "tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9e67464",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Binarizing sentiment scores\n",
    "tweets['pos'] = 0\n",
    "tweets['neg'] = 0\n",
    "\n",
    "tweets.loc[tweets['comp_score'] == 'pos', 'pos'] = 1\n",
    "tweets.loc[tweets['comp_score'] == 'neg', 'neg'] = 1\n",
    "\n",
    "tweets.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24da0eb8",
   "metadata": {},
   "source": [
    "### Visualizing Tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "288cda9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "figure = plt.figure(figsize=(5,5))\n",
    "sns.countplot(x='comp_score', data = tweets)\n",
    "plt.title('Tweets Polarity for the Top 100 Shows')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d107c44",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install wordcloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c07d1bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a0c12e4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pos_tweets = tweets[tweets['comp_score'] == 'pos']\n",
    "pos_tweets = pos_tweets.sort_values(['compound'], ascending = False)\n",
    "\n",
    "neg_tweets = tweets[tweets['comp_score'] == 'neg']\n",
    "neg_tweets = neg_tweets.sort_values(['compound'], ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "868020a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "text      = ' '.join([word for word in pos_tweets['text_clean']])\n",
    "plt.figure(figsize = (20,15), facecolor = 'None')\n",
    "wordcloud = WordCloud(max_words = 500, width = 1600, height = 800).generate(text)\n",
    "plt.imshow(wordcloud, interpolation = 'bilinear')\n",
    "plt.axis('off')\n",
    "plt.title('Most frequent words in posititve tweets', fontsize = 19)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1cf564d",
   "metadata": {},
   "outputs": [],
   "source": [
    "text      = ' '.join([word for word in neg_tweets['text_clean']])\n",
    "plt.figure(figsize = (20,15), facecolor = 'None')\n",
    "wordcloud = WordCloud(max_words = 500, width = 1600, height = 800).generate(text)\n",
    "plt.imshow(wordcloud, interpolation = 'bilinear')\n",
    "plt.axis('off')\n",
    "plt.title('Most frequent words in negative tweets', fontsize = 19)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c6fd1b0",
   "metadata": {},
   "source": [
    "### Calculating Score Per Show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46f1956e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by movie and calculate mean and standard deviation\n",
    "grouped_tweets         = tweets.groupby('show').agg({\n",
    "                                      'compound':       ['mean', 'std'],\n",
    "                                      'Favorite count': ['mean', 'std'],\n",
    "                                      'Retweet count':  ['mean', 'std']\n",
    "                                        })\n",
    "\n",
    "# Flatten the column names of the resulting dataframe\n",
    "grouped_tweets.columns = ['_'.join(col).strip() for col in grouped_tweets.columns.values]\n",
    "\n",
    "grouped_tweets['show'] = grouped_tweets.index\n",
    "\n",
    "grouped_tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf6c2169",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create bar plot\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.bar(grouped_tweets['show'], grouped_tweets['compound_mean'], color=grouped_tweets['compound_mean'].apply(lambda x: 'green' if x >= 0 else 'red'),width=0.5)\n",
    "plt.title('Twitter Sentiment Analysis for 100 Unique Shows',fontsize = 10)\n",
    "plt.xlabel('Show', fontsize = 10)\n",
    "plt.ylabel('Sentiment Score')\n",
    "plt.xticks(fontsize=5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca998934",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by movie and calculate mean and standard deviation\n",
    "grouped_tweets         = tweets.groupby('show').agg({\n",
    "                                      'compound':       ['mean', 'std'],\n",
    "                                      'Favorite count': ['mean', 'std'],\n",
    "                                      'Retweet count':  ['mean', 'std']\n",
    "                                        })\n",
    "\n",
    "# Flatten the column names of the resulting dataframe\n",
    "grouped_tweets.columns = ['_'.join(col).strip() for col in grouped_tweets.columns.values]\n",
    "\n",
    "grouped_tweets['show'] = grouped_tweets.index\n",
    "\n",
    "grouped_tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b258a53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the 'show' column\n",
    "grouped_tweets = grouped_tweets.drop('show', axis=1)\n",
    "\n",
    "# Reset index of grouped_tweets\n",
    "grouped_tweets = grouped_tweets.reset_index()\n",
    "\n",
    "# Drop the 'index' column\n",
    "grouped_tweets = grouped_tweets.drop('index', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df85b20d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge the two dataframes on the 'show' column\n",
    "df             = pd.merge(df, grouped_tweets, on='show')\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb567cdd",
   "metadata": {},
   "source": [
    "### Train and Test "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9d5b0ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The week_ending data type should be switched to datetime\n",
    "df['week_ending']      = pd.to_datetime(df['week_ending'])\n",
    "df['year']             = df['week_ending'].dt.year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35fed661",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the data before running the model\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ce4f340",
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop NA from columns\n",
    "df.dropna(subset=['compound_std'], inplace=True)\n",
    "df.dropna(subset=['Favorite count_std'], inplace=True)\n",
    "df.dropna(subset=['Retweet count_std'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d76b6225",
   "metadata": {},
   "outputs": [],
   "source": [
    "# finding the value that is in the mimddle of the dataframe\n",
    "median_date            = df['week_ending'].median()\n",
    "\n",
    "# splitting the dataset in 2 parts\n",
    "df_less_than_median    = df[df['week_ending']< median_date]\n",
    "df_greater_than_median = df[df['week_ending']>= median_date]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c114ca66",
   "metadata": {},
   "source": [
    "#### Regression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ae0bd63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regression model\n",
    "reg_model  = sm.OLS(df_less_than_median['weekly_gross'],\n",
    "               df_less_than_median[[  'week_number','seats_sold','avg_ticket_price','top_ticket_price','seats_in_theatre',\n",
    "                     'previews','compound_mean','compound_std','Favorite count_mean','Favorite count_std',\n",
    "                     'Retweet count_mean','Retweet count_std']])\n",
    "\n",
    "# fit the model and run the summary\n",
    "reg_model_result = reg_model.fit()\n",
    "print(reg_model_result.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a5419d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define predictors and dependent variable\n",
    "dependent_variable     = 'weekly_gross'\n",
    "predictors             = ['week_number','seats_sold','avg_ticket_price','top_ticket_price','seats_in_theatre',\n",
    "                          'previews','compound_mean','compound_std','Favorite count_mean','Favorite count_std',\n",
    "                          'Retweet count_mean','Retweet count_std']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a3e12ef",
   "metadata": {},
   "source": [
    "#### Random Forest and Lasso Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f17331e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initiate threge models\n",
    "rf    = RandomForestRegressor()\n",
    "lasso = Lasso()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5504bea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the models\n",
    "rf.fit    (df_less_than_median[predictors], df_less_than_median[dependent_variable])\n",
    "lasso.fit (df_less_than_median[predictors], df_less_than_median[dependent_variable])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08e7647a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions using df_greater_than_median\n",
    "rf_predictions        = rf.predict(df_greater_than_median[predictors])\n",
    "lasso_predictions     = lasso.predict(df_greater_than_median[predictors])\n",
    "reg_model_predictions = reg_model_result.predict(df_greater_than_median[predictors])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82877ae8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output \n",
    "# Calculate RMSE, initially calculate MSE and then take the root\n",
    "rf_mse         = mean_squared_error(df_greater_than_median[dependent_variable],rf_predictions)\n",
    "rf_rmse        = np.sqrt(rf_mse)\n",
    "\n",
    "lasso_mse      = mean_squared_error(df_greater_than_median[dependent_variable],lasso_predictions)\n",
    "lasso_rmse     = np.sqrt(lasso_mse)\n",
    "\n",
    "reg_model_mse  = mean_squared_error(df_greater_than_median[dependent_variable],reg_model_predictions)\n",
    "reg_model_rmse = np.sqrt(reg_model_mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e33f9d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the standard deviation of the whole thing\n",
    "rf_std         = np.std(rf_predictions)\n",
    "lasso_std      = np.std(lasso_predictions)\n",
    "reg_model_std  = np.std(reg_model_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5de35f42",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Output\n",
    "print(\"RMSE for Random Forest:\", rf_rmse)\n",
    "print(\"RMSE for Regression Model:\", lasso_rmse)\n",
    "print(\"RMSE for Regression Model:\", reg_model_rmse)\n",
    "print(\"Standard deviation for Random Forest:\", rf_std)\n",
    "print(\"Standard deviation for Lasso:\", lasso_std)\n",
    "print(\"Standard deviation for Regression Model:\", reg_model_std)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09aab226",
   "metadata": {},
   "source": [
    "### Visualising the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8057a0ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data for the bar plot\n",
    "rmse    = [rf_rmse, lasso_rmse, reg_model_rmse]\n",
    "std     = [rf_std, lasso_std, reg_model_std]\n",
    "models  = ['Random Forest', 'Lasso', 'Regression Model']\n",
    "\n",
    "# Create the bar plot\n",
    "fig, ax = plt.subplots()\n",
    "ax.bar(models, rmse, yerr=std, align='center', alpha=0.5, color = 'pink', ecolor='orange', capsize=10)\n",
    "ax.set_ylabel('RMSE and Standard deviation')\n",
    "plt.title('Comparing RMSE of 3 Models')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceb43a31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting Residuals\n",
    "lasso_residuals     = df_greater_than_median[dependent_variable] - lasso_predictions\n",
    "rf_residuals        = df_greater_than_median[dependent_variable] - rf_predictions\n",
    "reg_model_residuals = df_greater_than_median[dependent_variable] - reg_model_predictions\n",
    "\n",
    "# create a figure with 3 subplots\n",
    "fig, axs = plt.subplots(1, 3, figsize=(12,4))\n",
    "\n",
    "# Lasso Model\n",
    "axs[0].scatter(lasso_predictions, lasso_residuals, color = 'pink')\n",
    "axs[0].axhline(y=0, color='r', linestyle='-')\n",
    "axs[0].set_xlabel('Predicted values')\n",
    "axs[0].set_ylabel('Residuals')\n",
    "axs[0].set_title('Residual Plot for Lasso Model')\n",
    "\n",
    "# Random Forest Model\n",
    "axs[1].scatter(rf_predictions, rf_residuals, color = 'green')\n",
    "axs[1].axhline(y=0, color='r', linestyle='-')\n",
    "axs[1].set_xlabel('Predicted values')\n",
    "axs[1].set_ylabel('Residuals')\n",
    "axs[1].set_title('Residual Plot for Random Forest Model')\n",
    "\n",
    "# Regression Model\n",
    "axs[2].scatter(reg_model_predictions, reg_model_residuals, color = 'purple')\n",
    "axs[2].axhline(y=0, color='r', linestyle='-')\n",
    "axs[2].set_xlabel('Predicted values')\n",
    "axs[2].set_ylabel('Residuals')\n",
    "axs[2].set_title('Residual Plot for Regression Model')\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8206a163",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Actual vs Predicted Values\n",
    "actual = df_greater_than_median[dependent_variable]\n",
    "\n",
    "# Create a scatter plot\n",
    "plt.scatter(actual, lasso_predictions, color = 'pink')\n",
    "\n",
    "# Add axis labels and title\n",
    "plt.xlabel('Actual Weekly Gross')\n",
    "plt.ylabel('Predicted Weekly Gross')\n",
    "plt.title('Actual vs Predicted Weekly Gross, Lasso Model')\n",
    "\n",
    "# Add a diagonal line to show perfect predictions\n",
    "plt.plot([actual.min(), actual.max()], [actual.min(), actual.max()], 'k--', lw=3)\n",
    "\n",
    "# Display the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "461098a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(actual, rf_predictions, color = 'green')\n",
    "\n",
    "# Add axis labels and title\n",
    "plt.xlabel('Actual Weekly Gross')\n",
    "plt.ylabel('Predicted Weekly Gross')\n",
    "plt.title('Actual vs Predicted Weekly Gross, Random Forest Model')\n",
    "\n",
    "# Add a diagonal line to show perfect predictions\n",
    "plt.plot([actual.min(), actual.max()], [actual.min(), actual.max()], 'k--', lw=3)\n",
    "\n",
    "# Display the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f03ff7e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(actual, reg_model_predictions, color = 'purple')\n",
    "\n",
    "# Add axis labels and title\n",
    "plt.xlabel('Actual Weekly Gross')\n",
    "plt.ylabel('Predicted Weekly Gross')\n",
    "plt.title('Actual vs Predicted Weekly Gross, Regression Model')\n",
    "\n",
    "# Add a diagonal line to show perfect predictions\n",
    "plt.plot([actual.min(), actual.max()], [actual.min(), actual.max()], 'k--', lw=3)\n",
    "\n",
    "# Display the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58afac95",
   "metadata": {},
   "source": [
    "#### Identifying variables that increase RMSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da30afb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 10))\n",
    "sns.set(font_scale=0.5)\n",
    "sns.heatmap(df.corr(), annot=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21c9e815",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.boxplot(x = 'week_number', y = 'weekly_gross', data = df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1162b60e",
   "metadata": {},
   "outputs": [],
   "source": [
    "test2         = df.pivot_table(values = 'weekly_gross', index = 'week_number', columns = 'year')\n",
    "sns.heatmap(test2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "492be03b",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.lmplot( x = 'year', y = 'weekly_gross', data = df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "703d0480",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(df['compound_mean'], bins=10, color = 'pink')\n",
    "plt.title('Sentiment Analysis of Tweets about Climate Change')\n",
    "plt.xlabel('Sentiment Score')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccf0e85d",
   "metadata": {},
   "source": [
    "### Polynomial Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0df9d81b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "819e8987",
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform the data into polynomial features\n",
    "poly_features    = PolynomialFeatures(degree=3, include_bias=False)\n",
    "x_poly           = poly_features.fit_transform(df_less_than_median[predictors])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cf861d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train the polynomial regression model\n",
    "poly             = LinearRegression()\n",
    "poly.fit(x_poly, df_less_than_median[dependent_variable])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "952aa753",
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict values\n",
    "x_new_poly       = poly_features.transform(df_greater_than_median[predictors])\n",
    "poly_predictions = poly.predict(x_new_poly)\n",
    "\n",
    "\n",
    "poly_mse         = mean_squared_error(df_greater_than_median[dependent_variable], poly_predictions)\n",
    "poly_rmse        = np.sqrt(poly_mse)\n",
    "poly_std         = np.std(poly_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "732e180b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"RMSE for Polynomial Model:\", poly_rmse)\n",
    "print(\"STD for Polynomial Model:\", poly_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91cb84a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the data and the polynomial fit\n",
    "plt.plot(df_greater_than_median[predictors], poly_predictions, color='red')\n",
    "plt.xlabel(predictors)\n",
    "plt.ylabel(dependent_variable)\n",
    "plt.title('Polynomial Regression')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5635aee5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data for the bar plot\n",
    "rmse    = [rf_rmse, lasso_rmse, reg_model_rmse, poly_rmse]\n",
    "std     = [rf_std, lasso_std, reg_model_std, poly_std]\n",
    "models  = ['Random Forest', 'Lasso', 'Regression Model', 'Polynomial Model']\n",
    "\n",
    "# Create the bar plot\n",
    "fig, ax = plt.subplots()\n",
    "ax.bar(models, rmse, yerr=std, align='center', alpha=0.5, color = 'black', ecolor='orange', capsize=10)\n",
    "ax.set_ylabel('RMSE and Standard deviation')\n",
    "plt.title('Comparing RMSE of 4 Models, Polynomial Degree 3')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f13eb08",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(actual, poly_predictions, color = 'black')\n",
    "\n",
    "# Add axis labels and title\n",
    "plt.xlabel('Actual Weekly Gross')\n",
    "plt.ylabel('Predicted Weekly Gross')\n",
    "plt.title('Actual vs Predicted Weekly Gross, Polynomial Model')\n",
    "\n",
    "# Add a diagonal line to show perfect predictions\n",
    "plt.plot([actual.min(), actual.max()], [actual.min(), actual.max()], 'k--', lw=3)\n",
    "\n",
    "# Display the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d43b0294",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting Residuals\n",
    "poly_residuals = df_greater_than_median[dependent_variable] - poly_predictions\n",
    "\n",
    "# create a figure with 3 subplots\n",
    "fig, axs       = plt.subplots(1, 4, figsize=(16,4))\n",
    "\n",
    "# Lasso Model\n",
    "axs[0].scatter(lasso_predictions, lasso_residuals, color = 'pink')\n",
    "axs[0].axhline(y=0, color='r', linestyle='-')\n",
    "axs[0].set_xlabel('Predicted values')\n",
    "axs[0].set_ylabel('Residuals')\n",
    "axs[0].set_title('Residual Plot for Lasso Model')\n",
    "\n",
    "# Random Forest Model\n",
    "axs[1].scatter(rf_predictions, rf_residuals, color = 'green')\n",
    "axs[1].axhline(y=0, color='r', linestyle='-')\n",
    "axs[1].set_xlabel('Predicted values')\n",
    "axs[1].set_ylabel('Residuals')\n",
    "axs[1].set_title('Residual Plot for Random Forest Model')\n",
    "\n",
    "# Regression Model\n",
    "axs[2].scatter(reg_model_predictions, reg_model_residuals, color = 'purple')\n",
    "axs[2].axhline(y=0, color='r', linestyle='-')\n",
    "axs[2].set_xlabel('Predicted values')\n",
    "axs[2].set_ylabel('Residuals')\n",
    "axs[2].set_title('Residual Plot for Regression Model')\n",
    "\n",
    "# Polynomial Model\n",
    "axs[3].scatter(poly_predictions, poly_residuals, color = 'black')\n",
    "axs[3].axhline(y=0, color='r', linestyle='-')\n",
    "axs[3].set_xlabel('Predicted values')\n",
    "axs[3].set_ylabel('Residuals')\n",
    "axs[3].set_title('Residual Plot for Polynomial Model')\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
